{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be23edbb",
   "metadata": {},
   "source": [
    "# Socio-Temporal Risk Analysis & Prediction. (falta resumen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648f146a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Iniciando ejecucion del pipeline de entrenamiento V4.1...\n",
      "[INFO] Ejecutando fusion de datos (Data Enrichment)...\n",
      "[INFO] Generando estructuras temporales y calculo de volatilidad...\n",
      "[INFO] Iniciando ajuste de hiperparametros con 9 variables predictoras...\n",
      "\n",
      "[REPORTE] METRICAS DE RENDIMIENTO (V4.1 - Dataset Enriquecido):\n",
      "---------------------------------------------------------------\n",
      "[METRICA] Coeficiente de Determinacion (R2): 76.41%\n",
      "          -> Interpretacion: El modelo explica el 76.4% de la varianza en los datos criminales.\n",
      "\n",
      "[METRICA] Error Absoluto Medio (MAE):        1.9673\n",
      "          -> Interpretacion: Desviacion promedio de 1.97 incidentes respecto a la realidad.\n",
      "\n",
      "[INFO] Configuracion Final: 9 Variables (Incluye Volatilidad Estocastica).\n",
      "\n",
      "[EXITO] Modelo serializado y exportado correctamente en: modelo_homicidios_panama_socioeconomico_ULTRA.pkl\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PIPELINE DE ENTRENAMIENTO: ARQUITECTURA  (V4.1)FINAL\n",
    "# PROPOSITO: Entrenamiento de modelo predictivo basado en Bosques Aleatorios (Random Forest)\n",
    "# integrando componentes autorregresivos (Time-Series) y regresores exógenos (Socioeconomía).\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. MODULO DE INGESTA Y NORMALIZACIÓN DE DATOS (ETL)\n",
    "# ------------------------------------------------------------------------------\n",
    "# Objetivo: Cargar fuentes heterogéneas y asegurar la integridad referencial para el cruce de datos.\n",
    "\n",
    "print(\"[INFO] Iniciando ejecucion del pipeline de entrenamiento V4.1...\")\n",
    "\n",
    "# Carga de datasets desde rutas locales definidas\n",
    "# Se utilizan rutas absolutas para garantizar la localización de los insumos en el entorno de despliegue.\n",
    "ruta_dataset_crimen = 'C:/Users/Oliver/Downloads/demo1/src/Dataset_Homicidios_Panama_2017_2024_NormalizadoFINAL.xlsx'\n",
    "ruta_dataset_contexto = 'C:/Users/Oliver/Downloads/demo1/src/Datos_Contexto_Anual_MEJORADO.csv'\n",
    "\n",
    "df_crimen = pd.read_excel(ruta_dataset_crimen)\n",
    "df_contexto = pd.read_csv(ruta_dataset_contexto)\n",
    "\n",
    "# Estandarización de claves primarias (Primary Key Normalization)\n",
    "# Se aplica limpieza de cadenas (trim/upper) para garantizar que el JOIN entre datasets no pierda registros.\n",
    "df_crimen['PROVINCIA'] = df_crimen['PROVINCIA'].astype(str).str.upper().str.strip()\n",
    "df_contexto['PROVINCIA'] = df_contexto['PROVINCIA'].astype(str).str.upper().str.strip()\n",
    "\n",
    "print(\"[INFO] Ejecutando fusion de datos (Data Enrichment)...\")\n",
    "\n",
    "# Enriquecimiento del Dataset\n",
    "# Se utiliza un LEFT JOIN para anexar las variables socioeconómicas anuales a cada registro criminal.\n",
    "df_full = pd.merge(df_crimen, df_contexto, on=['AÑO', 'PROVINCIA'], how='left')\n",
    "df_full = df_full.fillna(0)  # Manejo de valores nulos mediante imputación a cero (Zero Imputation)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. INGENIERÍA DE CARACTERÍSTICAS (FEATURE ENGINEERING)\n",
    "# ------------------------------------------------------------------------------\n",
    "# Objetivo: Transformar datos crudos en vectores matemáticos densos que capturen \n",
    "# patrones temporales (Lags) y estructurales.\n",
    "\n",
    "print(\"[INFO] Generando estructuras temporales y calculo de volatilidad...\")\n",
    "\n",
    "# Reducción de Dimensionalidad (Aggregation)\n",
    "# Se agrupan los incidentes individuales para obtener una serie temporal discreta (Mes/Provincia).\n",
    "if 'contador' not in df_full.columns: df_full['contador'] = 1\n",
    "\n",
    "df_ml = df_full.groupby(['AÑO', 'MES', 'PROVINCIA']).agg({\n",
    "    'contador': 'sum',              # Variable Objetivo (Target): Volumen de criminalidad\n",
    "    'POBLACION_ESTIMADA': 'first',  # Variable Exógena Demográfica\n",
    "    'TASA_DESEMPLEO': 'first',      # Variable Exógena Económica\n",
    "    'INDICE_PANDILLAS': 'first'     # Variable Exógena de Seguridad\n",
    "}).reset_index()\n",
    "\n",
    "df_ml.rename(columns={'contador': 'TOTAL_HOMICIDIOS'}, inplace=True)\n",
    "\n",
    "# Transformación de Variable Temporal (Ordinal Encoding)\n",
    "# Convierte la representación textual de los meses a valores numéricos ordinales para preservar la secuencia temporal.\n",
    "mapa_meses = {'ENERO': 1, 'FEBRERO': 2, 'MARZO': 3, 'ABRIL': 4, 'MAYO': 5, 'JUNIO': 6,\n",
    "              'JULIO': 7, 'AGOSTO': 8, 'SEPTIEMBRE': 9, 'OCTUBRE': 10, 'NOVIEMBRE': 11, 'DICIEMBRE': 12}\n",
    "\n",
    "df_ml['MES'] = df_ml['MES'].astype(str).str.upper().str.strip()\n",
    "df_ml['MES_NUM'] = df_ml['MES'].map(mapa_meses)\n",
    "df_ml = df_ml.dropna(subset=['MES_NUM']) # Eliminación de registros con formato de fecha inválido\n",
    "\n",
    "# Codificación de Variables Categóricas (Label Encoding)\n",
    "# Transforma la variable nominal 'PROVINCIA' en códigos numéricos interpretables por el algoritmo.\n",
    "df_ml['PROVINCIA_CODE'] = df_ml['PROVINCIA'].astype('category').cat.codes\n",
    "\n",
    "# Ordenamiento cronológico estricto\n",
    "# Requerido para que las funciones de ventana (Rolling Windows) calculen correctamente el pasado.\n",
    "df_ml = df_ml.sort_values(by=['PROVINCIA', 'AÑO', 'MES_NUM'])\n",
    "\n",
    "# --- CONSTRUCCIÓN DE VARIABLES DE SERIES DE TIEMPO (LAG FEATURES) ---\n",
    "\n",
    "# 1. Autocorrelación de Primer Orden (Lag-1)\n",
    "# Captura la inercia inmediata: La correlación entre los eventos del mes actual y el mes previo.\n",
    "df_ml['HOMICIDIOS_MES_ANTERIOR'] = df_ml.groupby('PROVINCIA')['TOTAL_HOMICIDIOS'].shift(1)\n",
    "\n",
    "# 2. Tendencia Trimestral (Rolling Mean)\n",
    "# Suaviza el ruido mensual para identificar la tendencia subyacente de los últimos 3 meses (Moving Average).\n",
    "df_ml['PROMEDIO_TRIMESTRE'] = df_ml.groupby('PROVINCIA')['TOTAL_HOMICIDIOS'].transform(lambda x: x.rolling(3).mean().shift(1))\n",
    "\n",
    "# 3. Métrica de Volatilidad Estocástica (Rolling Std - NUEVO EN V4.0)\n",
    "# Calcula la desviación estándar móvil. Permite al modelo cuantificar la \"estabilidad\" o \"caos\" de la zona.\n",
    "# Zonas con alta volatilidad penalizan la confianza de la predicción puntual.\n",
    "df_ml['VOLATILIDAD_TRIMESTRE'] = df_ml.groupby('PROVINCIA')['TOTAL_HOMICIDIOS'].transform(lambda x: x.rolling(3).std().shift(1))\n",
    "\n",
    "# Limpieza final de vectores\n",
    "# Se imputan los primeros meses de la serie (que quedan vacíos por el desplazamiento) para no perder datos de entrenamiento.\n",
    "df_ml = df_ml.fillna(0)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. ENTRENAMIENTO Y OPTIMIZACIÓN DEL MODELO\n",
    "# ------------------------------------------------------------------------------\n",
    "# Algoritmo: Random Forest Regressor\n",
    "# Justificación: Modelo de ensamblaje no paramétrico, robusto a outliers y capaz de capturar relaciones no lineales.\n",
    "\n",
    "# Definición del Vector de Características (Feature Space - 9 Dimensiones)\n",
    "features = [\n",
    "    'AÑO', 'MES_NUM', 'PROVINCIA_CODE',         # Componentes Temporales/Espaciales\n",
    "    'HOMICIDIOS_MES_ANTERIOR',                  # Componente Autoregresivo (Lag)\n",
    "    'PROMEDIO_TRIMESTRE',                       # Componente de Tendencia\n",
    "    'VOLATILIDAD_TRIMESTRE',                    # Componente de Riesgo/Varianza\n",
    "    'POBLACION_ESTIMADA', 'TASA_DESEMPLEO', 'INDICE_PANDILLAS' # Regresores Socioeconómicos\n",
    "]\n",
    "\n",
    "X = df_ml[features]\n",
    "y = df_ml['TOTAL_HOMICIDIOS']\n",
    "\n",
    "# Partición de Datos (Hold-out Validation)\n",
    "# Se reserva un 20% de los datos para validación externa y detección de overfitting.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"[INFO] Iniciando ajuste de hiperparametros con {X.shape[1]} variables predictoras...\")\n",
    "\n",
    "# Configuración del Estimador con Regularización\n",
    "modelo_ultra = RandomForestRegressor(\n",
    "    n_estimators=300,       # Aumentado a 300 para reducir la varianza del error (Teorema del Límite Central)\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=4,     # Restricción: Cada hoja debe tener al menos 4 muestras (Suavizado de predicciones)\n",
    "    max_features='log2',    # Selección aleatoria de features para descorrelacionar los árboles\n",
    "    max_depth=None,\n",
    "    ccp_alpha=0.015,        # Cost-Complexity Pruning: Poda estadística para eliminar ramas ruidosas\n",
    "    bootstrap=True,\n",
    "    random_state=42,        # Semilla para reproducibilidad científica\n",
    "    n_jobs=-1               # Paralelización total en CPU\n",
    ")\n",
    "\n",
    "modelo_ultra.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. EVALUACIÓN DE DESEMPEÑO Y DESPLIEGUE\n",
    "# ------------------------------------------------------------------------------\n",
    "# Inferencia sobre el conjunto de prueba\n",
    "predicciones = modelo_ultra.predict(X_test)\n",
    "\n",
    "# Cálculo de Métricas de Bondad de Ajuste\n",
    "r2 = r2_score(y_test, predicciones)\n",
    "mae = mean_absolute_error(y_test, predicciones)\n",
    "\n",
    "print(f\"\"\"\n",
    "[REPORTE] METRICAS DE RENDIMIENTO (V4.1 - Dataset Enriquecido):\n",
    "---------------------------------------------------------------\n",
    "[METRICA] Coeficiente de Determinacion (R2): {r2:.2%}\n",
    "          -> Interpretacion: El modelo explica el {r2:.1%} de la varianza en los datos criminales.\n",
    "\n",
    "[METRICA] Error Absoluto Medio (MAE):        {mae:.4f}\n",
    "          -> Interpretacion: Desviacion promedio de {mae:.2f} incidentes respecto a la realidad.\n",
    "\n",
    "[INFO] Configuracion Final: 9 Variables (Incluye Volatilidad Estocastica).\n",
    "\"\"\")\n",
    "\n",
    "# Serialización del Objeto (Marshalling)\n",
    "# Se persiste el modelo entrenado en disco para su posterior consumo via API/Frontend.\n",
    "nombre_archivo = 'modelo_homicidios_panama_socioeconomico_ULTRA.pkl'\n",
    "joblib.dump(modelo_ultra, nombre_archivo)\n",
    "print(f\"[EXITO] Modelo serializado y exportado correctamente en: {nombre_archivo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e80f32e",
   "metadata": {},
   "source": [
    "Es importante señalar que las métricas de desempeño del modelo pueden presentar ligeras fluctuaciones controladas debido a la naturaleza estocástica del proceso de entrenamiento y generación de datos. Al incorporar ruido estadístico intencional en las variables socioeconómicas (como la actividad de pandillas), se simula la incertidumbre inherente a los fenómenos criminales del mundo real. Esta variabilidad garantiza que el sistema no se sobreajuste (overfitting) a un escenario estático, priorizando la robustez operativa y la capacidad de adaptación ante entornos dinámicos sobre una precisión teórica artificial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56580569",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
