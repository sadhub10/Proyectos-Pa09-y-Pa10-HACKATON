# ü¶æ CoreScripts ‚Äì Sistema de Asistencia Visual Inteligente

## üë• Nombre del equipo

**Equipo CoreScripts**

## üìå Integrantes del equipo

* **Gabriel Valderrama** ‚Äì Visi√≥n Artificial y Captura / Interpretaci√≥n Sem√°ntica
* **Gustavo De la Rivera** ‚Äì Visi√≥n Artificial y Captura / Interpretaci√≥n Sem√°ntica
* **Joel Monrroy** ‚Äì Lenguaje Natural, Texto a Voz, Integraci√≥n y Documentaci√≥n
* **Manuel Rojas** ‚Äì Lenguaje Natural, Texto a Voz, Integraci√≥n y Documentaci√≥n

---

## üéØ Nombre del proyecto

**VISUAL-VOICE ‚Äì Sistema de Asistencia Visual Inteligente**

---

## üß† Planteamiento del problema

Las personas con discapacidad visual enfrentan limitaciones significativas para desplazarse de forma aut√≥noma en entornos cotidianos, debido a la imposibilidad de identificar obst√°culos, personas u objetos cercanos. A pesar de los avances tecnol√≥gicos, muchas soluciones existentes son costosas, dependen de conexi√≥n a internet o requieren hardware especializado. Surge as√≠ la necesidad de desarrollar un sistema inteligente, accesible y de bajo costo que combine visi√≥n artificial con retroalimentaci√≥n auditiva en tiempo real para mejorar la orientaci√≥n, seguridad y autonom√≠a del usuario.

---

## üß© Descripci√≥n general del proyecto

VISUAL-VOICE es un prototipo de asistente visual que utiliza una c√°mara convencional y modelos de detecci√≥n de objetos para interpretar el entorno inmediato del usuario. El sistema procesa el video en tiempo real, identifica objetos relevantes, infiere su posici√≥n y distancia aproximada, y genera descripciones auditivas claras mediante t√©cnicas de Text-to-Speech (TTS). Todo el procesamiento se realiza localmente, priorizando la privacidad y el funcionamiento sin conexi√≥n.

---

## üèóÔ∏è Arquitectura del sistema

El sistema est√° organizado en m√≥dulos independientes pero integrados:

1. **Captura y percepci√≥n visual**: adquisici√≥n de video y detecci√≥n de objetos.
2. **Interpretaci√≥n sem√°ntica**: an√°lisis de resultados, reglas de prioridad, posici√≥n y distancia.
3. **Generaci√≥n de lenguaje natural**: construcci√≥n de mensajes comprensibles para el usuario.
4. **Salida auditiva (TTS)**: conversi√≥n de texto a voz y reproducci√≥n.
5. **Orquestaci√≥n e integraci√≥n**: coordinaci√≥n de m√≥dulos y control de flujo.

---

## üîÑ Flujo de funcionamiento

1. Activaci√≥n de la c√°mara.
2. Captura de frames en tiempo real.
3. Detecci√≥n de objetos mediante YOLO.
4. Filtrado y priorizaci√≥n de objetos relevantes.
5. Inferencia de posici√≥n (izquierda, frente, derecha) y distancia (cerca, media, lejos).
6. Generaci√≥n de descripciones en lenguaje natural.
7. Conversi√≥n de texto a audio y reproducci√≥n al usuario.

---

## üß™ Entorno de desarrollo

Todo el proyecto VISUAL-VOICE se ejecuta dentro de un entorno virtual de Python, con el objetivo de aislar dependencias, garantizar compatibilidad entre librer√≠as y facilitar la replicaci√≥n del sistema en otros equipos.

El uso de un entorno virtual permite:

* Evitar conflictos entre versiones de librer√≠as
* Mantener el proyecto organizado
* Facilitar la instalaci√≥n y ejecuci√≥n del sistema

---

## ‚ñ∂Ô∏è Instrucciones de ejecuci√≥n

### 1Ô∏è‚É£ Clonar el repositorio

```bash
cd visual-voice
```

### 2Ô∏è‚É£ Crear el entorno virtual

```bash
python -m venv venv
```

### 3Ô∏è‚É£ Activar el entorno virtual

**Windows:**

```bash
venv\Scripts\activate
```

**Linux / macOS:**

```bash
source venv/bin/activate
```

### 4Ô∏è‚É£ Instalar dependencias

```bash
pip install -r requirements.txt
```

### 5Ô∏è‚É£ Ejecutar el sistema

*Actualmente el proyecto se encuentra en fase de prototipo, por lo que no existe un √∫nico archivo principal de ejecuci√≥n. Se presentan m√≥dulos funcionales que demuestran la detecci√≥n, interpretaci√≥n y generaci√≥n de audio.*

Una vez finalizado, al ejecutarse el sistema:

* Activar√° la c√°mara
* Detectar√° objetos en tiempo real
* Generar√° descripciones auditivas del entorno

üìå **Nota:** Es necesario contar con una c√°mara funcional y permisos de acceso para su correcto funcionamiento.

---

## üéØ Objetivo general

Desarrollar un asistente visual inteligente capaz de detectar objetos en tiempo real mediante visi√≥n artificial y comunicar dicha informaci√≥n al usuario a trav√©s de descripciones auditivas claras.

### Objetivos espec√≠ficos

* Implementar detecci√≥n de objetos en tiempo real usando modelos de visi√≥n artificial.
* Interpretar sem√°nticamente la posici√≥n y distancia de los objetos detectados.
* Generar descripciones en lenguaje natural comprensibles para el usuario.
* Convertir dichas descripciones en audio mediante t√©cnicas de Text-to-Speech.
* Integrar todos los m√≥dulos en un sistema funcional.

---

## üß© Aplicaci√≥n general del proyecto

VISUAL-VOICE est√° dise√±ado como una herramienta de apoyo para personas con discapacidad visual, permiti√©ndoles reconocer su entorno inmediato a trav√©s de indicaciones auditivas. El sistema puede ser utilizado en interiores o exteriores, ayudando a identificar personas, obst√°culos y elementos relevantes para una navegaci√≥n m√°s segura.

---

## üõ†Ô∏è Herramientas y tecnolog√≠as utilizadas

* **Lenguaje de programaci√≥n:** Python
* **Visi√≥n artificial:** OpenCV
* **Modelo de detecci√≥n:** YOLO (Ultralytics ‚Äì v8)
* **Deep Learning:** PyTorch
* **Procesamiento sem√°ntico:** Reglas l√≥gicas personalizadas
* **Lenguaje natural:** Plantillas de generaci√≥n de texto
* **Text-to-Speech:** pyttsx3
* **Control de versiones:** Git y GitHub

---

## üìÇ Distribuci√≥n del trabajo y funciones

### üîπ Integrante 1 ‚Äì Visi√≥n Artificial y Captura

**Rol:** Responsable del m√≥dulo de percepci√≥n visual

**Funciones:**

* Configuraci√≥n del entorno de desarrollo (Python y librer√≠as)
* Captura de video en tiempo real con OpenCV
* Integraci√≥n del modelo YOLO
* Ajuste de clases de inter√©s (persona, silla, puerta, etc.)
* Visualizaci√≥n de bounding boxes
* Medici√≥n de latencia de detecci√≥n

**Entregables:**

* C√≥digo funcional de detecci√≥n
* Video demostrativo
* Explicaci√≥n t√©cnica

---

### üîπ Integrante 2 ‚Äì Interpretaci√≥n Sem√°ntica y L√≥gica

**Rol:** Responsable del razonamiento y reglas inteligentes

**Funciones:**

* An√°lisis de la salida del modelo YOLO
* Definici√≥n de reglas de posici√≥n (izquierda, frente, derecha)
* Estimaci√≥n de distancia seg√∫n el tama√±o del bounding box
* Priorizaci√≥n de objetos relevantes
* Generaci√≥n de estructura de datos sem√°ntica

**Ejemplo de salida:**

```json
{
  "objeto": "persona",
  "posicion": "frente",
  "distancia": "cerca"
}
```

**Entregables:**

* M√≥dulo sem√°ntico
* Documento de reglas
* Ejemplos de entrada y salida

---

### üîπ Integrante 3 ‚Äì Lenguaje Natural y Texto a Voz

**Rol:** Responsable de la comunicaci√≥n con el usuario

**Funciones:**

* Dise√±o de plantillas de generaci√≥n de texto
* Uni√≥n coherente de m√∫ltiples objetos
* Implementaci√≥n de Text-to-Speech
* Ajuste de velocidad y claridad del audio
* Pruebas en escenarios reales

**Ejemplo de salida:**

> ‚ÄúHay una persona frente a ti. A la derecha hay una silla.‚Äù

**Entregables:**

* Generador de texto
* M√≥dulo TTS integrado
* Audio de prueba

---

### üîπ Integrante 4 ‚Äì Integraci√≥n, Evaluaci√≥n y Documentaci√≥n

**Rol:** Coordinador t√©cnico y acad√©mico

**Funciones:**

* Integraci√≥n de todos los m√≥dulos
* Pruebas generales del sistema
* Definici√≥n de m√©tricas de evaluaci√≥n (precisi√≥n, latencia, usabilidad)
* Redacci√≥n del informe final
* Creaci√≥n del video demostrativo
* Preparaci√≥n de la presentaci√≥n

**Entregables:**

* Sistema completo funcional
* Informe final
* Video demostrativo
* Conclusiones y trabajo futuro

---

## ‚úÖ Resultado del proyecto

Como resultado, se obtuvo un sistema funcional capaz de detectar objetos en tiempo real, interpretar su ubicaci√≥n y comunicar esta informaci√≥n al usuario mediante descripciones auditivas claras. VISUAL-VOICE demuestra el potencial de la inteligencia artificial aplicada a la accesibilidad, ofreciendo una soluci√≥n viable, local y escalable para mejorar la autonom√≠a de personas con discapacidad visual.

---

## üìå Trabajo futuro

* Integraci√≥n con dispositivos m√≥viles
* Mejora en la estimaci√≥n de distancias
* Inclusi√≥n de reconocimiento de texto (OCR) y se√±ales
* Optimizaci√≥n del rendimiento en tiempo real
* Evaluaciones con usuarios finales

---

üìç *Proyecto desarrollado con fines acad√©micos.*
