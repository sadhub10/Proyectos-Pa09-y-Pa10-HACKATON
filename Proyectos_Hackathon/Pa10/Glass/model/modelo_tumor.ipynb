{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Montar drive"
      ],
      "metadata": {
        "id": "czvegK-5r0Yu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGPYu5G9rm5-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## revisar particion de datos"
      ],
      "metadata": {
        "id": "rNgRvvH6Cyeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dir_project = '/content/drive/MyDrive/projects/brainTumor'\n",
        "\n",
        "dir_dataset = os.path.join(dir_project,'dataset')\n",
        "\n",
        "train_path = 'train'\n",
        "val_path = 'val'\n",
        "\n",
        "train_dir = os.path.join(dir_dataset, train_path)\n",
        "val_dir = os.path.join(dir_dataset, val_path)\n",
        "\n",
        "def count_files_in_subdirectories(base_directory):\n",
        "    \"\"\"\n",
        "    Counts the number of files in each immediate subdirectory\n",
        "    of a given base directory.\n",
        "    \"\"\"\n",
        "    if not os.path.isdir(base_directory):\n",
        "        print(f\"Error: Directory not found at {base_directory}\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nCounting files in subdirectories of: {base_directory}\")\n",
        "    total_files = 0\n",
        "    for subdir_name in os.listdir(base_directory):\n",
        "        subdir_path = os.path.join(base_directory, subdir_name)\n",
        "        if os.path.isdir(subdir_path): # Ensure it's a directory\n",
        "            num_files = len([f for f in os.listdir(subdir_path) if os.path.isfile(os.path.join(subdir_path, f))])\n",
        "            print(f\"  {subdir_name}/: {num_files} files\")\n",
        "            total_files += num_files\n",
        "    print(f\"Total files in {base_directory}: {total_files}\")\n",
        "\n",
        "# Contar archivos train\n",
        "count_files_in_subdirectories(train_dir)\n",
        "\n",
        "# Contar archivos validation\n",
        "count_files_in_subdirectories(val_dir)\n",
        "\n",
        "# Para los directorios se usa la regla 80/20\n"
      ],
      "metadata": {
        "id": "IGwH2R3etKcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importar librerias"
      ],
      "metadata": {
        "id": "z5mK_b6_DKTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "from sklearn.utils import class_weight\n"
      ],
      "metadata": {
        "id": "P1fq3qd2CtB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparar datos"
      ],
      "metadata": {
        "id": "kBNBFZjxDVui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constantes de datos de entrada"
      ],
      "metadata": {
        "id": "pR9q43p0FDga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constantes para EfficientNetB0\n",
        "\n",
        "# imagenes\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# learning_rate\n",
        "learning_rate = 0.001\n",
        "\n",
        "# epochs\n",
        "epochs = 15\n",
        "\n",
        "# dropout\n",
        "DROPOUT = 0.5\n"
      ],
      "metadata": {
        "id": "5IH0CnyHDaoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## aumentar data de entranamiento"
      ],
      "metadata": {
        "id": "d-pHMLsGFHpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation para train\n",
        "train_image_generator = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# preprocessing para val\n",
        "validation_image_generator = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "train_data_gen = train_image_generator.flow_from_directory(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    directory=train_dir,\n",
        "    shuffle=True,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    class_mode='binary' # binario\n",
        ")\n",
        "\n",
        "val_data_gen = validation_image_generator.flow_from_directory(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    directory=val_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    class_mode='binary' # binario\n",
        ")\n"
      ],
      "metadata": {
        "id": "KjXy2LiDFJV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verificar clases"
      ],
      "metadata": {
        "id": "EkdpYLtGKTYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Verificación de Clases e Índices ---\")\n",
        "\n",
        "# Acceder al mapeo de class_indices\n",
        "class_indices = train_data_gen.class_indices\n",
        "print(f\"Mapeo de clases a índices: {class_indices}\")\n",
        "\n",
        "# 2. Obtener los nombres de las clases en el orden de los índices (para predicciones)\n",
        "# Esto es importante porque el modelo predice un índice (0, 1, 2...)\n",
        "# y se necesita saber a qué clase corresponde cada índice.\n",
        "num_classes = len(class_indices)\n",
        "class_names = [None] * num_classes # Crear una lista vacía con el tamaño correcto\n",
        "\n",
        "for class_name, index in class_indices.items():\n",
        "    class_names[index] = class_name\n",
        "\n",
        "print(f\"Nombres de las clases en el orden de los índices del modelo: {class_names}\")"
      ],
      "metadata": {
        "id": "mdoZ1rurKVPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calcular pesos"
      ],
      "metadata": {
        "id": "ySdo2-a7KjZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Calculando pesos de las clases para manejar el desbalance ---\")\n",
        "# Obtener los índices de clase de todas las imágenes en el conjunto de entrenamiento\n",
        "labels = train_data_gen.classes\n",
        "\n",
        "# Calcular los pesos de las clases\n",
        "weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced', # 'balanced' es la opción clave aquí\n",
        "    classes=np.unique(labels), # Asegura que todas las clases estén representadas\n",
        "    y=labels # Las etiquetas de clase de tus datos de entrenamiento\n",
        ")\n",
        "\n",
        "# Convertir el array de pesos a un diccionario, que es el formato que espera Keras\n",
        "class_weights = dict(enumerate(weights))\n",
        "\n",
        "print(f\"Pesos de las clases calculados: {class_weights}\")"
      ],
      "metadata": {
        "id": "v5ermu44KiLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargar modelo preentrenado EfficientNetB0"
      ],
      "metadata": {
        "id": "COQXrf8gGlav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cargar pre-entrenado EfficientNetB0\n",
        "base_model = EfficientNetB0(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
        "                            include_top=False,\n",
        "                            weights='imagenet')\n",
        "\n",
        "# congelar modelo base\n",
        "base_model.trainable = False\n",
        "\n",
        "# cabezera de capas personalizadas\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(DROPOUT)(x)\n",
        "# funcion de activacion sigmoide\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n"
      ],
      "metadata": {
        "id": "WsVbg3gIGtwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compilar modelo"
      ],
      "metadata": {
        "id": "pVlFJnMXLGJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "              loss='binary_crossentropy', # binario\n",
        "              metrics=['accuracy', Precision(name='precision'), Recall(name='recall'), AUC(name='auc')])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "KMSufKuXLIid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definir pasos y callbacks"
      ],
      "metadata": {
        "id": "5zYGTX5_IxJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mejor modelo\n",
        "log_dir = os.path.join(dir_project, \"logs\")\n",
        "pathModeloMejorPrecision = os.path.join(dir_project, 'modeloMejorPrecision.h5')\n",
        "\n",
        "# steps_per_epoch y validation_steps\n",
        "steps_per_epoch_train = train_data_gen.samples // BATCH_SIZE\n",
        "if train_data_gen.samples % BATCH_SIZE != 0:\n",
        "    steps_per_epoch_train += 1\n",
        "\n",
        "steps_per_epoch_val = val_data_gen.samples // BATCH_SIZE\n",
        "if val_data_gen.samples % BATCH_SIZE != 0:\n",
        "    steps_per_epoch_val += 1\n",
        "\n",
        "# Callbacks para mejorar entrenamiento, principalmente precision\n",
        "callbacks = [\n",
        "    # EarlyStopping: Monitorear 'val_precision' y mejores pesos\n",
        "    EarlyStopping(monitor='val_precision',\n",
        "                  patience=5, # cuando para\n",
        "                  mode='max', # 'max' precision\n",
        "                  restore_best_weights=True,\n",
        "                  verbose=1),\n",
        "\n",
        "    # ModelCheckpoint: guarda modelo con mejor 'val_precision'\n",
        "    ModelCheckpoint(filepath=pathModeloMejorPrecision,\n",
        "                    monitor='val_precision',\n",
        "                    save_best_only=True,\n",
        "                    mode='max', # 'max' precision\n",
        "                    verbose=1),\n",
        "\n",
        "    # ReduceLROnPlateau: monitorea y reduce precision\n",
        "    ReduceLROnPlateau(monitor='val_precision',\n",
        "                      factor=0.2, # Reducir learning rate 20%\n",
        "                      patience=3, # cuando para\n",
        "                      mode='max',\n",
        "                      min_lr=0.00001, # learning rate minimo\n",
        "                      verbose=1),\n",
        "]\n"
      ],
      "metadata": {
        "id": "9-rsFPjHIzZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento"
      ],
      "metadata": {
        "id": "PmjmWdR3JKP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenar modelo base EfficientNetB0"
      ],
      "metadata": {
        "id": "6R-R5nnxIZ8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo base"
      ],
      "metadata": {
        "id": "KIkTN4nSbWSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Iniciando entrenamiento...\")\n",
        "# cambiar nombre history_base\n",
        "history_base = model.fit(\n",
        "    train_data_gen,\n",
        "    # Pasos por epoca train\n",
        "    steps_per_epoch=steps_per_epoch_train,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        "    # Pasos por epoca validación\n",
        "    validation_steps=steps_per_epoch_val,\n",
        "    class_weight=class_weights, # pesos por clase para evitar desbalance\n",
        "    callbacks=callbacks # callbacks optimiza entrenamiento\n",
        ")\n",
        "print(\"Entrenamiento finalizado.\")\n"
      ],
      "metadata": {
        "id": "vkfdaBzsIZIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cargar modelo base"
      ],
      "metadata": {
        "id": "vC2JbSgibbjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el mejor modelo guardado de la fase de extracción de características\n",
        "try:\n",
        "    model = tf.keras.models.load_model(pathModeloMejorPrecision)\n",
        "    print(f\"Mejor modelo de extracción de características cargado desde: {pathModeloMejorPrecision}\")\n",
        "except Exception as e:\n",
        "    print(f\"No se pudo cargar el mejor modelo de extracción de características. Continuando con el modelo actual. Error: {e}\")\n"
      ],
      "metadata": {
        "id": "dViloHyubhzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento Fine-Tuning"
      ],
      "metadata": {
        "id": "pnwiwLnSb3pM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configurar parametros Fine-Tuning"
      ],
      "metadata": {
        "id": "E9QmeDq82Vou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Descongelar el modelo base\n",
        "base_model.trainable = True\n",
        "\n",
        "# 2. Determinar el punto de corte\n",
        "# EfficientNetB0 tiene ~237 capas, descongelar a partir de la capa 200\n",
        "# para ajustar los últimos bloques de extracción de características de alto nivel.\n",
        "FINE_TUNE_AT_LAYER = 200\n",
        "\n",
        "# Congelar todas las capas antes del punto de corte\n",
        "for layer in base_model.layers[:FINE_TUNE_AT_LAYER]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Mantener las capas de BatchNormalization en modo inferencia\n",
        "# Esto es vital en EfficientNet para no desestabilizar el entrenamiento.\n",
        "for layer in base_model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
        "        layer.trainable = False\n",
        "\n",
        "print(f\"Capas totales en el modelo base: {len(base_model.layers)}\")\n",
        "print(f\"Capas entrenables tras descongelar: {len(model.trainable_variables)}\")\n",
        "\n",
        "# 3. Re-compilar con una tasa de aprendizaje extremadamente baja\n",
        "fine_tune_learning_rate = 1e-5 # 0.00001\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=fine_tune_learning_rate, clipnorm=1.0),\n",
        "    loss='binary_crossentropy', # Riguroso para 2 clases\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.Precision(name='precision'),\n",
        "        tf.keras.metrics.Recall(name='recall'),\n",
        "        tf.keras.metrics.AUC(name='auc')\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "94sNjRZK2Uuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Callbacks para Fine-Tuning"
      ],
      "metadata": {
        "id": "fuj2tf4h3BBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_fine_tuning = 10\n",
        "# Recuperar última época de la fase de entrenamiento base\n",
        "initial_epoch_fine_tune = len(history_base.history['accuracy'])\n",
        "\n",
        "checkpoint_filepath_ft = os.path.join(dir_project, 'modeloFineTuneBrainTumor.h5')\n",
        "\n",
        "callbacks_ft = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_precision', patience=5, mode='max', restore_best_weights=True),\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath_ft, monitor='val_precision', save_best_only=True, mode='max'),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_precision', factor=0.2, patience=2, min_lr=1e-7)\n",
        "]\n"
      ],
      "metadata": {
        "id": "PzBIcGd73KqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento Fine-Tuning"
      ],
      "metadata": {
        "id": "QyHV8hnv3r-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Iniciando ajuste fino desde la época {initial_epoch_fine_tune}...\")\n",
        "\n",
        "history_fine_tuning = model.fit(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=steps_per_epoch_train,\n",
        "    epochs=initial_epoch_fine_tune + epochs_fine_tuning,\n",
        "    initial_epoch=initial_epoch_fine_tune,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=steps_per_epoch_val,\n",
        "    callbacks=callbacks_ft\n",
        ")\n"
      ],
      "metadata": {
        "id": "JJA95Gim3vT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cargar modelo tras Fine Tuning"
      ],
      "metadata": {
        "id": "cFj_Obdf5prk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el mejor modelo guardado de la fase de ajuste fino (este será el modelo final)\n",
        "try:\n",
        "    final_model = tf.keras.models.load_model(checkpoint_filepath_ft)\n",
        "    print(f\"Modelo final cargado (mejor de ajuste fino): {checkpoint_filepath_ft}\")\n",
        "except Exception as e:\n",
        "    print(f\"No se pudo cargar el mejor modelo de ajuste fino. Usando el modelo actual. Error: {e}\")\n",
        "    final_model = model # Si falla, usa el modelo tal como quedó al final del entrenamiento\n"
      ],
      "metadata": {
        "id": "Pa8fw4-U5uvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Guardar modelo"
      ],
      "metadata": {
        "id": "D5NG64w66t7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el modelo entrenado\n",
        "# antes h5, luego keras por integracion con fastapi\n",
        "path_final_keras_model = os.path.join(dir_project, 'modeloBrainTumor.keras')\n",
        "final_model.save(path_final_keras_model)\n",
        "print(f\"Modelo Keras final guardado exitosamente en: {path_final_keras_model}\")\n"
      ],
      "metadata": {
        "id": "xtR03rC36xWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metricas y Evaluacion"
      ],
      "metadata": {
        "id": "1B7_SDd-67OU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Librerias de evaluacion\n",
        "\n"
      ],
      "metadata": {
        "id": "Rfv0Qx-V_p8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "9oXBbFND62uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargar modelo y generar predicciones"
      ],
      "metadata": {
        "id": "zbjftBmh_stv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el mejor modelo obtenido\n",
        "final_model = tf.keras.models.load_model(checkpoint_filepath_ft)\n",
        "\n",
        "# Generar predicciones\n",
        "val_data_gen.reset()\n",
        "y_true = val_data_gen.classes\n",
        "# Predicciones probabilísticas\n",
        "preds = final_model.predict(val_data_gen, verbose=1)\n",
        "# Convertir a clases binarias (0 o 1) usando umbral 0.5\n",
        "y_pred = (preds > 0.5).astype(int).ravel()\n",
        "\n",
        "class_names = list(val_data_gen.class_indices.keys())\n"
      ],
      "metadata": {
        "id": "hOwXWfLG_06_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Informe de clasificacion"
      ],
      "metadata": {
        "id": "HLzwjyia_8Ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Informe de Clasificación ---\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"INFORME DE CLASIFICACIÓN FINAL\")\n",
        "print(\"=\"*40)\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n"
      ],
      "metadata": {
        "id": "aQiagj7O_-r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matriz de confusion"
      ],
      "metadata": {
        "id": "LtQgfgzoACAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Matriz de Confusión ---\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Matriz de Confusión: Detección de Tumor')\n",
        "plt.xlabel('Predicción del Modelo')\n",
        "plt.ylabel('Realidad (Gold Standard)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RRa5YxCxAEe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graficas de rendimiento"
      ],
      "metadata": {
        "id": "jro3fdriA3KX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_full_history(h1, h2):\n",
        "    acc = h1.history['accuracy'] + h2.history['accuracy']\n",
        "    val_acc = h1.history['val_accuracy'] + h2.history['val_accuracy']\n",
        "    loss = h1.history['loss'] + h2.history['loss']\n",
        "    val_loss = h1.history['val_loss'] + h2.history['val_loss']\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Gráfica de Accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(acc, label='Entrenamiento')\n",
        "    plt.plot(val_acc, label='Validación')\n",
        "    plt.axvline(x=len(h1.history['accuracy'])-1, color='r', linestyle='--', label='Inicio Fine-tuning')\n",
        "    plt.title('Precisión (Accuracy)')\n",
        "    plt.legend()\n",
        "\n",
        "    # Gráfica de Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(loss, label='Entrenamiento')\n",
        "    plt.plot(val_loss, label='Validación')\n",
        "    plt.axvline(x=len(h1.history['loss'])-1, color='r', linestyle='--', label='Inicio Fine-tuning')\n",
        "    plt.title('Pérdida (Loss)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_full_history(history_base, history_fine_tuning)\n"
      ],
      "metadata": {
        "id": "OYLb_qsmA2rK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}